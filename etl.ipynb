{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "import os\n",
    "import logging\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import DateType, LongType, IntegerType\n",
    "from pyspark.sql.functions import udf, col, lit, year, month, upper, to_date\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.config(\"spark.jars.repositories\", \"https://repos.spark-packages.org/\"\n",
    "                                   ).config(\"spark.jars.packages\", \"saurfang:spark-sas7bdat:2.0.0-s_2.11\"\n",
    "                                           ).enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_columns(table, new_columns):\n",
    "    for original, new in zip(table.columns, new_columns):\n",
    "        table = table.withColumnRenamed(original, new)\n",
    "    return table\n",
    "\n",
    "def SAS_to_date(date):\n",
    "    if date is not None:\n",
    "        return pd.to_timedelta(date, unit='D') + pd.Timestamp('1960-1-1')\n",
    "    \n",
    "sas_to_date_udf = udf(SAS_to_date, DateType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the resulting dim_demographics table to parquet files in the output data location\n",
    "output_path = 'transformed_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"Start processing US Cities Demographies\")\n",
    "# Construct the full path to the demographic data file stored in the input data location\n",
    "us_cities_demographics_data = 'us-cities-demographics.csv'\n",
    "\n",
    "# Read the demographic data file using Spark's DataFrame API with CSV format\n",
    "# The CSV file has a header row and uses a semicolon (';') as a delimiter\n",
    "df = spark.read.format('csv').options(header=True, delimiter=';').load(us_cities_demographics_data)\n",
    "\n",
    "# Group the data and sum the 'Count' for each combination of index columns and 'Race'\n",
    "grouped_df = df.groupBy(\n",
    "    'City', 'State', 'Median Age', 'Male Population', 'Female Population', 'Total Population',\n",
    "    'Number of Veterans', 'Foreign-born', 'Average Household Size', 'State Code', 'Race'\n",
    ").agg(F.sum('Count').alias('Count'))\n",
    "\n",
    "# Pivot the DataFrame on the 'Race' column, creating a column for each unique 'Race' value\n",
    "pivoted_df = grouped_df.groupBy(\n",
    "    'City', 'State', 'Median Age', 'Male Population', 'Female Population', 'Total Population',\n",
    "    'Number of Veterans', 'Foreign-born', 'Average Household Size', 'State Code'\n",
    ").pivot('Race').agg(F.first('Count'))\n",
    "\n",
    "# Replace any nulls with 0, since `fill_value=0` was used in Pandas\n",
    "final_df = pivoted_df.fillna(0)\n",
    "\n",
    "# Cast all relevant columns from float to int\n",
    "final_df = final_df.withColumn(\"Median Age\", F.col(\"Median Age\").cast(IntegerType())) \\\n",
    "    .withColumn(\"Male Population\", F.col(\"Male Population\").cast(IntegerType())) \\\n",
    "    .withColumn(\"Female Population\", F.col(\"Female Population\").cast(IntegerType())) \\\n",
    "    .withColumn(\"Total Population\", F.col(\"Total Population\").cast(IntegerType())) \\\n",
    "    .withColumn(\"Number of Veterans\", F.col(\"Number of Veterans\").cast(IntegerType())) \\\n",
    "    .withColumn(\"Foreign-born\", F.col(\"Foreign-born\").cast(IntegerType())) \\\n",
    "    .withColumn(\"Average Household Size\", F.col(\"Average Household Size\").cast(IntegerType())) \\\n",
    "    .withColumn(\"American Indian and Alaska Native\", F.col(\"American Indian and Alaska Native\").cast(IntegerType())) \\\n",
    "    .withColumn(\"Asian\", F.col(\"Asian\").cast(IntegerType())) \\\n",
    "    .withColumn(\"Black or African-American\", F.col(\"Black or African-American\").cast(IntegerType())) \\\n",
    "    .withColumn(\"Hispanic or Latino\", F.col(\"Hispanic or Latino\").cast(IntegerType())) \\\n",
    "    .withColumn(\"White\", F.col(\"White\").cast(IntegerType()))\n",
    "\n",
    "# Define a list of new column names for renaming\n",
    "new_columns = ['city', 'state', 'median_age', 'male_population', 'female_population','total_population', 'num_vetarans', 'foreign_born',\n",
    "               'avg_house_size', 'state_code', 'american_indian', 'asian', 'black_african_american', 'hispanic', 'white']\n",
    "\n",
    "# Rename the columns of the DataFrame using a custom function 'rename_columns'\n",
    "dim_demographics = rename_columns(final_df, new_columns)\n",
    "\n",
    "# The mode 'overwrite' will replace any existing files with the same name\n",
    "#dim_demographics.write.mode(\"overwrite\").parquet(f'{output_path}/dim_demographics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------+----------+---------------+-----------------+----------------+------------+------------+--------------+----------+---------------+-----+----------------------+--------+------+\n",
      "|         city|         state|median_age|male_population|female_population|total_population|num_vetarans|foreign_born|avg_house_size|state_code|american_indian|asian|black_african_american|hispanic| white|\n",
      "+-------------+--------------+----------+---------------+-----------------+----------------+------------+------------+--------------+----------+---------------+-----+----------------------+--------+------+\n",
      "|       Skokie|      Illinois|        43|          31382|            33437|           64819|        1066|       27424|             2|        IL|              0|20272|                  4937|    6590| 40642|\n",
      "|    Charlotte|North Carolina|        34|         396646|           430475|          827121|       36046|      128897|             2|        NC|           8746|55399|                301568|  113731|446795|\n",
      "|   Manchester| New Hampshire|        37|          54845|            55378|          110223|        5473|       14506|             2|        NH|            558| 4304|                  6896|   11962|100108|\n",
      "|        Chico|    California|        29|          46168|            44168|           90336|        4519|        8425|             2|        CA|           2766| 6101|                  3164|   15578| 80467|\n",
      "|Silver Spring|      Maryland|        33|          40601|            41862|           82463|        1562|       30908|             2|        MD|           1084| 8841|                 21330|   25924| 37756|\n",
      "+-------------+--------------+----------+---------------+-----------------+----------------+------------+------------+--------------+----------+---------------+-----+----------------------+--------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the result (optional)\n",
    "dim_demographics.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"Start processing label descriptions\")\n",
    "label_file = 'I94_SAS_Labels_Descriptions.SAS'\n",
    "\n",
    "with open(label_file) as f:\n",
    "    contents = f.readlines()\n",
    "\n",
    "dim_port_code = {}\n",
    "for ports in contents[302:962]:\n",
    "    pair = ports.split('=')\n",
    "    port_code, port = pair[0].strip(\"\\t\").strip().strip(\"'\"),pair[1].strip('\\t').strip().strip(\"''\")\n",
    "    dim_port_code[port_code] = port\n",
    "\n",
    "# Create a Spark DataFrame from the dictionary\n",
    "df_port_code = spark.createDataFrame(dim_port_code.items(), ['port_code', 'port'])\n",
    "\n",
    "# Save the DataFrame as Parquet (overwrite mode)\n",
    "#df_port_code.write.mode(\"overwrite\").parquet(f'{output_path}/dim_port_code')  # Save as Parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+\n",
      "|port_code|                port|\n",
      "+---------+--------------------+\n",
      "|      ALC|ALCAN, AK        ...|\n",
      "|      ANC|ANCHORAGE, AK    ...|\n",
      "|      BAR|BAKER AAF - BAKER...|\n",
      "|      DAC|DALTONS CACHE, AK...|\n",
      "|      PIZ|DEW STATION PT LA...|\n",
      "+---------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_port_code.show(5)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_state_code = {}\n",
    "for states in contents[982:1036]:\n",
    "    pair = states.split('=')\n",
    "    state_code, state = pair[0].strip('\\t').strip(\"'\"), pair[1].strip().strip(\"'\")\n",
    "    dim_state_code[state_code] = state\n",
    "\n",
    "# Create a Spark DataFrame from the dictionary\n",
    "df_state_code = spark.createDataFrame(dim_state_code.items(), ['state_code', 'state'])\n",
    "\n",
    "# Save the DataFrame as Parquet (overwrite mode)\n",
    "#df_state_code.write.mode(\"overwrite\").parquet(f'{output_path}/dim_state_code')  # Save as Parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|state_code|     state|\n",
      "+----------+----------+\n",
      "|        AK|    ALASKA|\n",
      "|        AZ|   ARIZONA|\n",
      "|        AR|  ARKANSAS|\n",
      "|        CA|CALIFORNIA|\n",
      "|        CO|  COLORADO|\n",
      "+----------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_state_code.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_visa_code = {}\n",
    "for visas in contents[1046:1049]:\n",
    "    pair = visas.split('=')\n",
    "    visa_code, visa_category = pair[0].strip(), pair[1].strip().strip(\"'\")\n",
    "    dim_visa_code[visa_code] = visa_category\n",
    "\n",
    "# Create a Spark DataFrame from the dictionary    \n",
    "df_visa_code = spark.createDataFrame(dim_visa_code.items(), ['visa_code', 'visa_category'])\n",
    "\n",
    "# Save the DataFrame as Parquet (overwrite mode)\n",
    "#df_visa_code.write.mode(\"overwrite\").parquet(f'{output_path}/dim_visa_code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------+\n",
      "|visa_code|visa_category|\n",
      "+---------+-------------+\n",
      "|        1|     Business|\n",
      "|        2|     Pleasure|\n",
      "|        3|      Student|\n",
      "+---------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_visa_code.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_transport_mode_code = {}\n",
    "for mode in contents[972:976]:\n",
    "    pair = mode.split('=')\n",
    "    transport_mode_code, transport_mode = pair[0].strip('\\t').strip(\"'\"), pair[1].strip().strip(\"'\")\n",
    "    dim_transport_mode_code[transport_mode_code] = transport_mode\n",
    "\n",
    "df_transport_mode_code = spark.createDataFrame(dim_transport_mode_code.items(), ['transport_mode_code', 'transport_mode'])\n",
    "\n",
    "#df_transport_mode_code.write.mode(\"overwrite\").parquet(f'{output_path}/dim_transport_mode_code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------------+\n",
      "|transport_mode_code| transport_mode|\n",
      "+-------------------+---------------+\n",
      "|                 1 |            Air|\n",
      "|                 2 |            Sea|\n",
      "|                 3 |           Land|\n",
      "|                 9 |Not reported' ;|\n",
      "+-------------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_transport_mode_code.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_country_code = {}\n",
    "for countries in contents[9:298]:\n",
    "    pair = countries.split('=')\n",
    "    country_code, country = pair[0].strip(), pair[1].strip().strip(\"'\")\n",
    "    dim_country_code[country_code] = country\n",
    "\n",
    "df_country_code = spark.createDataFrame(dim_country_code.items(),['country_code', 'country'])\n",
    "\n",
    "#df_country_code.write.mode(\"overwrite\").parquet(f'{output_path}/dim_country_code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+\n",
      "|country_code|             country|\n",
      "+------------+--------------------+\n",
      "|         582|MEXICO Air Sea, a...|\n",
      "|         236|         AFGHANISTAN|\n",
      "|         101|             ALBANIA|\n",
      "|         316|             ALGERIA|\n",
      "|         102|             ANDORRA|\n",
      "+------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_country_code.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read immigration data file\n",
    "immigration_data = ('../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat')\n",
    "df = spark.read.format('com.github.saurfang.sas.spark').load(immigration_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"Start processing fact_immigration\")\n",
    "# extract columns to create fact_immigration table\n",
    "fact_immigration = df.select('cicid', 'i94yr', 'i94mon', 'i94port', 'i94addr', 'arrdate', 'depdate', 'i94mode', 'i94visa').distinct()\n",
    "\n",
    "# data wrangling to match data model\n",
    "new_columns = ['cic_id', 'arrival_year', 'arrival_month', 'arrival_port_code', 'arrival_state_code', 'arrival_date','departure_date', 'transport_mode_code', 'visa_code']\n",
    "fact_immigration = rename_columns(fact_immigration, new_columns)\n",
    "\n",
    "fact_immigration = fact_immigration.withColumn('arrival_date', sas_to_date_udf(col('arrival_date')))\n",
    "fact_immigration = fact_immigration.withColumn('departure_date', sas_to_date_udf(col('departure_date')))\n",
    "\n",
    "# Convert float columns to integer columns\n",
    "fact_immigration = fact_immigration \\\n",
    "    .withColumn('cic_id', col('cic_id').cast('int')) \\\n",
    "    .withColumn('arrival_year', col('arrival_year').cast('int')) \\\n",
    "    .withColumn('arrival_month', col('arrival_month').cast('int')) \\\n",
    "    .withColumn('transport_mode_code', col('transport_mode_code').cast('int')) \\\n",
    "    .withColumn('visa_code', col('visa_code').cast('int'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write fact_immigration table to parquet files partitioned by state and city\n",
    "#fact_immigration.write.mode(\"overwrite\").partitionBy('arrival_state_code').parquet(f'{output_path}/fact_immigration')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------+-------------+-----------------+------------------+------------+--------------+-------------------+---------+\n",
      "|cic_id|arrival_year|arrival_month|arrival_port_code|arrival_state_code|arrival_date|departure_date|transport_mode_code|visa_code|\n",
      "+------+------------+-------------+-----------------+------------------+------------+--------------+-------------------+---------+\n",
      "|    27|        2016|            4|              BOS|                MA|  2016-04-01|    2016-04-05|                  1|        1|\n",
      "|   233|        2016|            4|              NYC|                NY|  2016-04-01|    2016-04-07|                  1|        2|\n",
      "|  1103|        2016|            4|              NEW|                NY|  2016-04-01|    2016-04-09|                  1|        2|\n",
      "|  1123|        2016|            4|              NEW|                PA|  2016-04-01|    2016-04-08|                  1|        1|\n",
      "|  1446|        2016|            4|              NYC|                NY|  2016-04-01|    2016-04-07|                  1|        2|\n",
      "+------+------------+-------------+-----------------+------------------+------------+--------------+-------------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fact_immigration.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"Start processing dim_immigrants\")\n",
    "\n",
    "# Extract columns to create dim_immigrants table\n",
    "dim_immigrants = df.select('cicid', 'i94cit', 'i94res', 'biryear', 'gender').distinct()\n",
    "\n",
    "# Data wrangling to match data model\n",
    "new_columns = ['cic_id', 'citizen_country', 'residence_country', 'birth_year', 'gender']\n",
    "dim_immigrants = rename_columns(dim_immigrants, new_columns)\n",
    "\n",
    "# Convert float columns to int\n",
    "dim_immigrants = dim_immigrants.withColumn('cic_id', col('cic_id').cast('int')) \\\n",
    "                               .withColumn('citizen_country', col('citizen_country').cast('int')) \\\n",
    "                               .withColumn('residence_country', col('residence_country').cast('int')) \\\n",
    "                               .withColumn('birth_year', col('birth_year').cast('int'))\n",
    "\n",
    "# Write dim_immigrants table to parquet files\n",
    "#dim_immigrants.write.mode(\"overwrite\").parquet(f'{output_path}/dim_immigrants')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------------+-----------------+----------+------+\n",
      "|cic_id|citizen_country|residence_country|birth_year|gender|\n",
      "+------+---------------+-----------------+----------+------+\n",
      "|    16|            101|              101|      1988|  null|\n",
      "|    84|            103|              103|      1994|     M|\n",
      "|   536|            103|              103|      1956|     M|\n",
      "|   670|            103|              124|      1979|     M|\n",
      "|   681|            103|              112|      1955|     F|\n",
      "+------+---------------+-----------------+----------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dim_immigrants.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"Start processing dim_immi_airline\")\n",
    "\n",
    "# Extract columns to create dim_immi_airline table\n",
    "dim_immi_airline = df.select('cicid', 'airline', 'admnum', 'fltno', 'visatype').distinct()\n",
    "\n",
    "# Data wrangling to match data model\n",
    "new_columns = ['cic_id', 'airline', 'admin_num', 'flight_number', 'visa_type']\n",
    "dim_immi_airline = rename_columns(dim_immi_airline, new_columns)\n",
    "\n",
    "# Convert float columns to int or long\n",
    "dim_immi_airline = dim_immi_airline.withColumn('cic_id', col('cic_id').cast('int')) \\\n",
    "                                   .withColumn('admin_num', col('admin_num').cast(LongType()))\n",
    "\n",
    "# Write dim_immi_airline table to parquet files\n",
    "#dim_immi_airline.write.mode(\"overwrite\").parquet(f'{output_path}/dim_immi_airline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+-----------+-------------+---------+\n",
      "|cic_id|airline|  admin_num|flight_number|visa_type|\n",
      "+------+-------+-----------+-------------+---------+\n",
      "|   372|     OS|55428239233|        00097|       WT|\n",
      "|   498|     OS|55428626033|        00065|       WT|\n",
      "|   715|     AA|55453491633|        00237|       WT|\n",
      "|   719|     AA|55439268633|        00717|       WT|\n",
      "|   993|     UA|55413424033|        00056|       WT|\n",
      "+------+-------+-----------+-------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dim_immi_airline.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"Start processing dim_temperature\")\n",
    "# read temperature data file\n",
    "temperature_data = '../../data2/GlobalLandTemperaturesByCity.csv'\n",
    "df = spark.read.csv(temperature_data, header=True)\n",
    "\n",
    "df = df.where(df['Country'] == 'United States')\n",
    "df = df.withColumn('dt', to_date(col('dt')))\n",
    "df = df.withColumn('year', year(df['dt']))\n",
    "df = df.withColumn('month', month(df['dt']))\n",
    "\n",
    "dim_temperature = df.select(['AverageTemperature', 'AverageTemperatureUncertainty', 'City', 'Country', 'year', 'month']).distinct()\n",
    "\n",
    "new_columns = ['avg_temp', 'avg_temp_uncertnty', 'city', 'country', 'year', 'month']\n",
    "dim_temperature = rename_columns(dim_temperature, new_columns)\n",
    "\n",
    "# write dim_temperature table to parquet files\n",
    "#dim_temperature.write.mode(\"overwrite\").parquet(f'{output_path}/dim_temperature')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------------------+-------+-------------+----+-----+\n",
      "|          avg_temp| avg_temp_uncertnty|   city|      country|year|month|\n",
      "+------------------+-------------------+-------+-------------+----+-----+\n",
      "|            27.517|              1.101|Abilene|United States|1872|    7|\n",
      "|            27.275|              1.693|Abilene|United States|1883|    7|\n",
      "|21.311999999999998|              0.226|Abilene|United States|1925|    5|\n",
      "|            16.561|0.35700000000000004|Abilene|United States|1930|   10|\n",
      "|             8.354|              0.239|Abilene|United States|1977|   12|\n",
      "+------------------+-------------------+-------+-------------+----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dim_temperature.show(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
